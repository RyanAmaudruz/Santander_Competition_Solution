{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e1b3c5a8fdc4ca68ae56ad534561db6edf1a4c8"
   },
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "781412441f8479a3a3a596dd413edf189346c8ab"
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "bd525288aeaab0e4886254df001b78ef11460c88"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../input/train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0d7b6db56903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read train, test and submission files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msub_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../input/train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Read train, test and submission files\n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "sub_df = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06eab4fdafabbbd9a37fa14814be25cdaf9f17e3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print out summary info about the train set\n",
    "print('train set shape: {}'.format(train_df.shape))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2469afae23c4230b82e3b754b5cb4eeb5fa43c47",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print out summary info about the test set\n",
    "print('test set shape: {}'.format(test_df.shape))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f467ce9cab415de223ce54e5a74c1283ccd845a5"
   },
   "source": [
    "## Filter out the fake samples from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "279add5196828b2b497ac5ada6362743d578b803",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_beautiful_test(test):\n",
    "    \n",
    "    test_rnd = np.round(test.iloc[:, 1:], 2)\n",
    "    \n",
    "    ugly_indexes = []\n",
    "    non_ugly_indexes = []\n",
    "\n",
    "    for idx in tqdm(range(len(test))):\n",
    "        if not np.all(test_rnd.iloc[idx, :].values==test.iloc[idx, 1:].values):\n",
    "            ugly_indexes.append(idx)\n",
    "        else:\n",
    "            non_ugly_indexes.append(idx)\n",
    "    \n",
    "    print('Count of real samples in the test set: {}'.format(len(non_ugly_indexes)))\n",
    "    print('Count of fake samples in the test set: {}'.format(len(ugly_indexes)))\n",
    "\n",
    "    np.save('test_ugly_indexes', np.array(ugly_indexes))\n",
    "    np.save('test_non_ugly_indexes', np.array(non_ugly_indexes))\n",
    "\n",
    "    test = test.iloc[non_ugly_indexes].reset_index(drop=True)\n",
    "\n",
    "    return test, non_ugly_indexes, ugly_indexes\n",
    "\n",
    "# Get a test set free of fake samples\n",
    "test_df, non_ugly_indexes, ugly_indexes = get_beautiful_test(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4dbc2f0b3998bc8c1b6dcda17f4fa7b94de5290"
   },
   "source": [
    "## Combine the train and test set into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80b09332b57b76f17f49a2627e16c00ffa9922ec",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the target columns in the test set\n",
    "test_df['target'] = train_df['target'].mean()\n",
    "\n",
    "# Update the test set's index\n",
    "test_df.index = [i for i in range(len(train_df), len(train_df) + len(test_df))]\n",
    "\n",
    "# Reorder the test set columns\n",
    "test_df = test_df[train_df.columns.tolist()]\n",
    "\n",
    "# Concatenate the train and test set\n",
    "df = pd.concat([train_df, test_df])\n",
    "\n",
    "print('full dataset shape: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f798df366a17c019695ae37f283777f8810b0d7f"
   },
   "source": [
    "# Finding the leak columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6af53615e283b149c4d7de62d0ee6b058474c620"
   },
   "source": [
    "At around the middle of the competition timeframe, the leaderboard suddenly showed significant improvements from a top score around 1.35 to score below 1.0. This suggested that the dataset contained leaked targets. Not long after, Giba shared the leak with all players in one of his kernels: https://www.kaggle.com/titericz/the-property-by-giba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5966fd3e174359da0c6d49529cb5ab3d245f10a"
   },
   "source": [
    "The magic columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31357652bd246cfcbebb1258351919a5c0261eb2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the magic columns\n",
    "cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1',\n",
    "        '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9',\n",
    "        'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b',\n",
    "        '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212', '66ace2992',\n",
    "        'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', '1931ccfdd',\n",
    "        '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a',\n",
    "        '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2', '0572565c2',\n",
    "        '190db8488', 'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aea7e1a77a34cefb49822cf13250290b59a2fedf"
   },
   "source": [
    "Using the magic columns, we can create an algorithm that finds the order of the rows. After that, we can use the ordered rows to find more column sets and iteratively find more ordered rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b92ab18c0a507b43426c1b374ae61a590f69760a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_column_sets(df, cols):\n",
    "    \n",
    "    # Create a dictionary to store the row sets for each lags\n",
    "    lag_row_dic = {}\n",
    "    lag = 1\n",
    "    \n",
    "    # Order the row indexes such that we start the search with those containing the most values\n",
    "    df_rows = df.copy()[cols]\n",
    "    df_rows['count'] = df_rows.astype(bool).sum(axis=1)\n",
    "    df_rows = df_rows.sort_values('count', ascending=False)\n",
    "    rows_copy = df_rows.index.tolist()\n",
    "    \n",
    "    # Recover memory\n",
    "    del df_rows\n",
    "    gc.collect()\n",
    "    \n",
    "    # Store the column set in a list\n",
    "    col_set_list = [cols]\n",
    "    \n",
    "    # Create a trigger to stop the loop once no new columns are found\n",
    "    column_trigger = True\n",
    "    previous_column_count = 0\n",
    "\n",
    "    while column_trigger:\n",
    "        \n",
    "        # Make a copy of the row list\n",
    "        rows = rows_copy.copy()\n",
    "        \n",
    "        # Get 2 dfs that are offset by the lag\n",
    "        cols_a = [col for col_set in col_set_list for col in col_set[:-lag]]\n",
    "        cols_b = [col for col_set in col_set_list for col in col_set[lag:]]\n",
    "\n",
    "        # Convert the dfs to numpy arrays\n",
    "        df_a = df[cols_a].values\n",
    "        df_b = df[cols_b].values\n",
    "\n",
    "        # Create a list to store the row sets\n",
    "        row_set_list = []\n",
    "        count = 0\n",
    "\n",
    "        # Check row by row if we can find a sequence\n",
    "        while len(rows) > 0:\n",
    "\n",
    "            # Add the row to the set\n",
    "            row_set = [rows[0]]\n",
    "\n",
    "            # Delete the rows from the rows to select\n",
    "            rows = [i for j, i in enumerate(rows) if i != row_set[0]]\n",
    "\n",
    "            # Get the vector to test the row\n",
    "            a = df_a[row_set[0]]\n",
    "            b = df_b[row_set[0]]\n",
    "\n",
    "            # Finding if a new row follows the current row, if so add them to the row set. \n",
    "            # The new row becomes the current row and we add the rows iteratively\n",
    "            \n",
    "            trigger = True\n",
    "            \n",
    "            while np.sum(np.apply_along_axis(np.all, 1, np.equal(a, df_b))) == 1 and trigger:\n",
    "                \n",
    "                # Get the new row\n",
    "                new_row = np.where(np.apply_along_axis(np.all, 1, np.equal(a, df_b)))[0][0]\n",
    "                \n",
    "                # This validates that the new potential row could not be used for another row\n",
    "                if np.sum(np.apply_along_axis(np.all, 1, np.equal(df_b[new_row], df_a))) == 1:\n",
    "                    \n",
    "                    # Add the new row\n",
    "                    row_set = row_set + [new_row]\n",
    "\n",
    "                    # Update the current rows with the new rows\n",
    "                    a = df_a[new_row]\n",
    "\n",
    "                    # Remove the new found row from the rows to check\n",
    "                    rows = [i for j, i in enumerate(rows) if i != new_row]\n",
    "\n",
    "                else:\n",
    "                    # This means there are more than one possible candidate, the loop should be stopped\n",
    "                    print('New row {} could not be added - double'.format(new_row))\n",
    "                    trigger = False\n",
    "    \n",
    "            trigger = True\n",
    "\n",
    "            # Same as above, but for rows that precedes\n",
    "            while np.sum(np.apply_along_axis(np.all, 1, np.equal(b, df_a))) == 1 and trigger:\n",
    "\n",
    "                # Get the new row\n",
    "                new_row = np.where(np.apply_along_axis(np.all, 1, np.equal(b, df_a)))[0][0]\n",
    "\n",
    "                # This validates that the new potential row could not be used for another row\n",
    "                if np.sum(np.apply_along_axis(np.all, 1, np.equal(df_a[new_row], df_b))) == 1:\n",
    "                    \n",
    "                    # Add the new row\n",
    "                    row_set = [new_row] + row_set\n",
    "\n",
    "                    # Update the current rows with the new rows\n",
    "                    b = df_b[new_row]\n",
    "\n",
    "                    # Remove the new found row from the rows to check\n",
    "                    rows = [i for j, i in enumerate(rows) if i != int(new_row)]\n",
    "\n",
    "                else:\n",
    "                    # This means there are more than one possible candidate, the loop should be stopped\n",
    "                    print('New row {} could not be added - double'.format(new_row))\n",
    "                    trigger = False\n",
    "\n",
    "            # Add the sequence of rows to the row set list\n",
    "            row_set_list.append(row_set)\n",
    "\n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                print('\\n{}'.format(time.strftime('%H:%M')))\n",
    "                print('Completed {} set!'.format(count))\n",
    "                print('{} rows remaining!'.format(len(rows)))\n",
    "\n",
    "        lag_row_dic['lag_{}'.format(lag)] = row_set_list\n",
    "\n",
    "        # Using the row sequences we found, we look for more column sets\n",
    "\n",
    "        # Get the row set previously found\n",
    "        row_set_list2 = lag_row_dic['lag_{}'.format(1)]\n",
    "        row_set_list2 = [row_set for row_set in row_set_list2 if len(row_set) > 2]\n",
    "\n",
    "        # Get the rows for each offset\n",
    "        rows_a = [row for rowset in row_set_list2 for row in rowset[:-1]]\n",
    "        rows_b = [row for rowset in row_set_list2 for row in rowset[1:]]\n",
    "\n",
    "        # Get a list of columns\n",
    "        col_list = [col for col in df.columns if (col != 'target' and col != 'ID')]\n",
    "\n",
    "        # Get two numpy arrays with each offset\n",
    "        df_a = df.copy()[col_list].iloc[rows_a].values\n",
    "        df_b = df.copy()[col_list].iloc[rows_b].values\n",
    "\n",
    "        # Get a list of column indexes to look through\n",
    "        col_index_list = [i for i in range(df_a.shape[1])]\n",
    "        \n",
    "        # Create variables for the log and to store the sequences of columns\n",
    "        count = 0\n",
    "        col_set_count = 0\n",
    "        col_set_list = []\n",
    "\n",
    "        # Loop through all columns\n",
    "        while len(col_index_list) > 0:\n",
    "\n",
    "            # Add the current column to the set\n",
    "            col_index = col_index_list[0]\n",
    "            col_set = [col_list[col_index]]\n",
    "\n",
    "            # Delete the cols from the cols to select\n",
    "            col_index_list = [i for j, i in enumerate(col_index_list) if i != col_index]\n",
    "\n",
    "            # Get a column to check both if a column precedes or follows the current one\n",
    "            a = df_a[:, col_index]\n",
    "            b = df_b[:, col_index]\n",
    "\n",
    "            trigger = True\n",
    "            \n",
    "            # Add successive columns iteratively (similar process as for rows)\n",
    "            while np.sum(np.apply_along_axis(np.all, 1, np.equal(a, df_b.T))) == 1 and trigger:\n",
    "\n",
    "                new_col_index = np.where(\n",
    "                    np.apply_along_axis(np.all, 1, np.equal(a, df_b.T))\n",
    "                )[0][0]\n",
    "\n",
    "                if np.sum(np.apply_along_axis(\n",
    "                    np.all, 1, np.equal(df_b[:, new_col_index], df_a.T)\n",
    "                )) == 1:\n",
    "\n",
    "                    col_set = col_set + [col_list[new_col_index]]\n",
    "\n",
    "                    a = df_a[:, new_col_index]\n",
    "\n",
    "                    col_index_list = [\n",
    "                        i for j, i in enumerate(col_index_list) if i != new_col_index\n",
    "                    ]\n",
    "\n",
    "                else:\n",
    "                    trigger = False\n",
    "\n",
    "            trigger = True\n",
    "\n",
    "            while np.sum(np.apply_along_axis(\n",
    "                np.all, 1, np.equal(b, df_a.T)\n",
    "            )) == 1 and trigger:\n",
    "\n",
    "                new_col_index = np.where(\n",
    "                    np.apply_along_axis(np.all, 1, np.equal(b, df_a.T))\n",
    "                )[0][0]\n",
    "\n",
    "                if np.sum(np.apply_along_axis(\n",
    "                    np.all, 1, np.equal(df_a[:, new_col_index], df_b.T)\n",
    "                )) == 1:\n",
    "\n",
    "                    col_set = [col_list[new_col_index]] + col_set\n",
    "\n",
    "                    b = df_b[:, new_col_index]\n",
    "\n",
    "                    col_index_list = [\n",
    "                        i for j, i in enumerate(col_index_list) if i != new_col_index]\n",
    "\n",
    "                else:\n",
    "                    trigger = False\n",
    "\n",
    "            # Count the full column sets\n",
    "            if len(col_set) == 40:\n",
    "                col_set_count += 1\n",
    "            \n",
    "            # Add the sequence of columns found to the col set list\n",
    "            col_set_list.append(col_set)\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "            # Print out log\n",
    "            if count % 10 == 0:\n",
    "                print('\\n{}'.format(time.strftime('%H:%M')))\n",
    "                print('\\nCompleted {} set!'.format(count))\n",
    "                print('{} columns remaining!'.format(len(col_index_list)))\n",
    "                print('{} full sets found!'.format(col_set_count))\n",
    "                   \n",
    "        # Get the sets of at least 2 columns\n",
    "        final_col_set_list = [col_set for col_set in col_set_list if len(col_set)>1]\n",
    "        \n",
    "        # Store the sets of 40 columns (complete sets)\n",
    "        final_col_set_list2 = [col_set for col_set in col_set_list if len(col_set)==40]\n",
    "    \n",
    "        # Store the new number of column sets\n",
    "        new_column_set_count = len(final_col_set_list2)\n",
    "\n",
    "        # If no new columns are found, we stop the loop\n",
    "        # Otherwise, we go through a new loop with the new column sets\n",
    "        if new_column_set_count > previous_column_count:\n",
    "            previous_column_count = new_column_set_count\n",
    "        else:\n",
    "            column_trigger = False\n",
    "    \n",
    "    return lag_row_dic, final_col_set_list, final_col_set_list2\n",
    "\n",
    "lag_row_dic, final_col_set_list, final_col_set_list2 = find_column_sets(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61b7890127b551592de84d28fd015d9dd3ccc492",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb349b42931cc2a77ee421cfa84b79293b216ee0",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c3afa034f4c699eb987bd653c87b59f8f43aa70",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
